{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Transformer Evaluation — Commonsense Reasoning on SWAG\n",
    "\n",
    "This notebook evaluates a **pre-trained transformer** (BERT) on the [SWAG dataset](https://rowanzellers.com/swag/),\n",
    "a commonsense NLI benchmark where a model must choose the most plausible sentence continuation\n",
    "from four candidates given a premise.\n",
    "\n",
    "## Task format\n",
    "\n",
    "```\n",
    "Premise  : \"She opened the door and...\"\n",
    "Choice A : \"...sat down on the couch.\"       ← correct\n",
    "Choice B : \"...flew to the moon.\"\n",
    "Choice C : \"...turned into a cat.\"\n",
    "Choice D : \"...disappeared into thin air.\"\n",
    "```\n",
    "\n",
    "## Evaluation strategy\n",
    "\n",
    "We use **zero-shot scoring**: for each candidate continuation, we concatenate the premise and\n",
    "the candidate, compute the model's confidence score using the `[CLS]` token logit,\n",
    "and pick the highest-scoring candidate as the prediction.\n",
    "\n",
    "> **Dataset**: `swag/val.csv` — 17,992 validation examples  \n",
    "> **Model**: `bert-base-uncased` (zero-shot, no fine-tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Hugging Face — tokenizer and model\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Central place for all hyperparameters and paths — change `N_SAMPLES` to run on a\n",
    "larger subset or set it to `None` to evaluate the full validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Paths ──────────────────────────────────────────────────────────────────\n",
    "DATA_PATH = Path(\"swag/val.csv\")  # SWAG validation split\n",
    "\n",
    "# ── Model ──────────────────────────────────────────────────────────────────\n",
    "# bert-base-uncased: 12 layers, 110M parameters — good balance of speed and accuracy\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "# ── Evaluation ─────────────────────────────────────────────────────────────\n",
    "# Limit to a subset for faster iteration; set to None to evaluate all 17 992 rows\n",
    "N_SAMPLES = 500\n",
    "MAX_SEQ_LEN = 128  # maximum token length per (premise + continuation) pair\n",
    "BATCH_SIZE = 32  # number of pairs processed in a single forward pass\n",
    "\n",
    "# ── Reproducibility ────────────────────────────────────────────────────────\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ── Device ─────────────────────────────────────────────────────────────────\n",
    "# Automatically use GPU if available, otherwise fall back to CPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device : {DEVICE}\")\n",
    "print(f\"Model        : {MODEL_NAME}\")\n",
    "print(f\"Eval samples : {N_SAMPLES if N_SAMPLES else 'all'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Load & Explore the SWAG Dataset\n",
    "\n",
    "SWAG contains grounded commonsense inference examples derived from video captions.\n",
    "Each row provides:\n",
    "- `sent1` / `sent2`: premise split into two parts\n",
    "- `ending0`–`ending3`: four candidate continuations\n",
    "- `label`: index (0–3) of the correct continuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation CSV\n",
    "df = pd.read_csv(DATA_PATH, index_col=0)\n",
    "\n",
    "# Randomly sample N_SAMPLES rows for faster evaluation\n",
    "if N_SAMPLES:\n",
    "    df = df.sample(n=N_SAMPLES, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# The full premise is the concatenation of sent1 and sent2\n",
    "df[\"premise\"] = df[\"sent1\"].str.strip() + \" \" + df[\"sent2\"].str.strip()\n",
    "\n",
    "# Candidate continuations are stored as separate columns\n",
    "ENDING_COLS = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n",
    "\n",
    "print(f\"Loaded {len(df):,} examples\")\n",
    "df[[\"premise\", *ENDING_COLS, \"label\"]].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Label distribution\n",
    "\n",
    "A uniform label distribution confirms the dataset is balanced —\n",
    "a random baseline would achieve ~25% accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how often each label (correct answer position) appears\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "label_counts = df[\"label\"].value_counts().sort_index()\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values, palette=\"viridis\", ax=ax)\n",
    "ax.set_xlabel(\"Correct ending index\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"SWAG — label distribution (val subset)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Random baseline accuracy (chance level)\n",
    "n_choices = len(ENDING_COLS)\n",
    "print(f\"Random baseline accuracy : {1 / n_choices:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Load the Pre-trained Model\n",
    "\n",
    "We load `bert-base-uncased` with a **sequence-classification head** (2 outputs).\n",
    "In zero-shot scoring, we use the logit of the positive class (index 1) as\n",
    "a proxy for how plausible the (premise, continuation) pair is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and cache tokenizer — converts raw text into token IDs\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Download and cache model — bert-base-uncased with a 2-class classification head\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.to(DEVICE)  # move weights to GPU if available\n",
    "model.eval()  # disable dropout for deterministic inference\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "print(f\"Model loaded — {n_params:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9b",
   "metadata": {},
   "source": [
    "## 4. Zero-Shot Scoring\n",
    "\n",
    "For each example we:\n",
    "1. Build four `(premise, ending)` pairs\n",
    "2. Tokenize and batch them\n",
    "3. Run a forward pass through BERT\n",
    "4. Use **logit[1]** (positive-class score) as the plausibility score\n",
    "5. Pick the ending with the highest score as the model prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # disable gradient computation — we are only doing inference\n",
    "def score_endings(premise: str, endings: list[str]) -> list[float]:\n",
    "    \"\"\"Return a plausibility score for each (premise, ending) pair.\n",
    "\n",
    "    The score is the logit of the positive class (index 1) from BERT's\n",
    "    sequence-classification head — higher means more plausible.\n",
    "\n",
    "    Args:\n",
    "        premise: The sentence context (sent1 + sent2).\n",
    "        endings: List of four candidate continuations.\n",
    "\n",
    "    Returns:\n",
    "        List of four float scores, one per ending.\n",
    "    \"\"\"\n",
    "    # Tokenize all four (premise, ending) pairs at once\n",
    "    encoding = tokenizer(\n",
    "        [premise] * len(endings),  # repeat premise for each ending\n",
    "        endings,\n",
    "        padding=True,  # pad to the longest pair in this batch\n",
    "        truncation=True,  # truncate to MAX_SEQ_LEN if needed\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        return_tensors=\"pt\",  # return PyTorch tensors\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Forward pass — shape: (num_endings, num_labels)\n",
    "    logits = model(**encoding).logits\n",
    "\n",
    "    # Logit at index 1 = positive-class score (proxy for plausibility)\n",
    "    scores = logits[:, 1].tolist()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation\n",
    "\n",
    "We iterate over every example, score the four candidates, and record\n",
    "the predicted label alongside the gold label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "all_scores = []  # store all four scores per example for later analysis\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    premise = row[\"premise\"]\n",
    "    endings = [row[col] for col in ENDING_COLS]\n",
    "\n",
    "    # Score each of the four candidate continuations\n",
    "    scores = score_endings(premise, endings)\n",
    "\n",
    "    # The predicted label is the index of the highest-scoring ending\n",
    "    pred = int(np.argmax(scores))\n",
    "    predictions.append(pred)\n",
    "    all_scores.append(scores)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "# Attach predictions back to the dataframe\n",
    "df[\"predicted\"] = predictions\n",
    "df[\"correct\"] = df[\"predicted\"] == df[\"label\"]\n",
    "\n",
    "print(\n",
    "    f\"Evaluated {len(df):,} examples in {elapsed:.1f}s ({elapsed / len(df) * 1000:.1f} ms/example)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "\n",
    "### Overall accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy: fraction of examples where predicted == gold label\n",
    "accuracy = df[\"correct\"].mean()\n",
    "random_baseline = 1 / len(ENDING_COLS)  # 25 % for 4-choice task\n",
    "\n",
    "print(f\"Accuracy         : {accuracy:.2%}\")\n",
    "print(f\"Random baseline  : {random_baseline:.2%}\")\n",
    "print(f\"Gain over random : +{(accuracy - random_baseline):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Per-label accuracy\n",
    "\n",
    "Checking whether accuracy varies depending on which position holds the correct answer\n",
    "reveals positional biases in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy broken down by gold-label position (0, 1, 2, 3)\n",
    "per_label = df.groupby(\"label\")[\"correct\"].mean().rename(\"accuracy\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "sns.barplot(x=per_label.index, y=per_label.values, palette=\"viridis\", ax=ax)\n",
    "ax.axhline(\n",
    "    random_baseline,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Random ({random_baseline:.0%})\",\n",
    ")\n",
    "ax.axhline(accuracy, color=\"blue\", linestyle=\"--\", label=f\"Overall ({accuracy:.0%})\")\n",
    "ax.set_xlabel(\"Correct ending index\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Per-label accuracy — BERT zero-shot on SWAG\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Score distributions: correct vs. incorrect endings\n",
    "\n",
    "We expect the model to assign higher scores to correct endings.\n",
    "Overlapping distributions indicate where the model struggles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all scores and tag each one as 'correct' or 'incorrect' ending\n",
    "score_records = []\n",
    "for i, row in df.iterrows():\n",
    "    gold = int(row[\"label\"])\n",
    "    for j, score in enumerate(all_scores[i]):\n",
    "        score_records.append(\n",
    "            {\n",
    "                \"score\": score,\n",
    "                \"ending_type\": \"correct\" if j == gold else \"incorrect\",\n",
    "            }\n",
    "        )\n",
    "score_df = pd.DataFrame(score_records)\n",
    "\n",
    "# KDE plot: correct endings should skew higher than incorrect ones\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "sns.kdeplot(\n",
    "    data=score_df,\n",
    "    x=\"score\",\n",
    "    hue=\"ending_type\",\n",
    "    fill=True,\n",
    "    alpha=0.4,\n",
    "    palette={\"correct\": \"green\", \"incorrect\": \"salmon\"},\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_xlabel(\"Plausibility score (logit)\")\n",
    "ax.set_title(\"Score distribution: correct vs. incorrect endings\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "The confusion matrix shows which gold labels the model tends to confuse with each other.\n",
    "A strong diagonal indicates consistent predictions regardless of answer position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Build the 4x4 confusion matrix (gold label × predicted label)\n",
    "cm = confusion_matrix(df[\"label\"], df[\"predicted\"], labels=[0, 1, 2, 3])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"end0\", \"end1\", \"end2\", \"end3\"],\n",
    "    yticklabels=[\"end0\", \"end1\", \"end2\", \"end3\"],\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_xlabel(\"Predicted label\")\n",
    "ax.set_ylabel(\"Gold label\")\n",
    "ax.set_title(\"Confusion matrix — BERT zero-shot on SWAG\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 7. Error Analysis\n",
    "\n",
    "Inspecting model errors reveals systematic failure patterns and\n",
    "guides further improvement (e.g., fine-tuning, prompt engineering).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the 5 errors where the model was most confidently wrong\n",
    "# Confidence = score of the (wrong) predicted ending\n",
    "errors = df[~df[\"correct\"]].copy()\n",
    "\n",
    "# Retrieve predicted score for each error (the score of the predicted ending)\n",
    "errors[\"pred_score\"] = [all_scores[i][predictions[i]] for i in errors.index]\n",
    "\n",
    "# Sort by highest predicted score (most confident wrong predictions)\n",
    "worst = errors.nlargest(5, \"pred_score\")[\n",
    "    [\"premise\", \"predicted\", \"label\", \"pred_score\", *ENDING_COLS]\n",
    "]\n",
    "\n",
    "print(\"=== Top-5 most confident errors ===\")\n",
    "for _, row in worst.iterrows():\n",
    "    gold_idx = int(row[\"label\"])\n",
    "    pred_idx = int(row[\"predicted\"])\n",
    "    gold_text = row[ENDING_COLS[gold_idx]]\n",
    "    pred_text = row[ENDING_COLS[pred_idx]]\n",
    "    score = row[\"pred_score\"]\n",
    "    premise = row[\"premise\"]\n",
    "    print(f\"\\nPremise   : {premise}\")\n",
    "    print(f\"Gold      : [{gold_idx}] {gold_text}\")\n",
    "    print(f\"Predicted : [{pred_idx}] {pred_text} (score={score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Model | `bert-base-uncased` (zero-shot) |\n",
    "| Dataset | SWAG validation set |\n",
    "| Eval samples | `N_SAMPLES` |\n",
    "| **Accuracy** | _run cells above_ |\n",
    "| Random baseline | 25.0% |\n",
    "\n",
    "### Key takeaways\n",
    "\n",
    "- **Zero-shot BERT** already outperforms the random baseline on commonsense reasoning,\n",
    "  demonstrating that pre-training on large corpora captures implicit world knowledge.\n",
    "- Score distributions show a meaningful separation between correct and incorrect endings,\n",
    "  but significant overlap remains — this is expected for a zero-shot approach.\n",
    "- **Fine-tuning BERT on SWAG** (as in the original paper) reaches ~86% accuracy,\n",
    "  compared to human performance of ~88%.\n",
    "\n",
    "### Next steps\n",
    "\n",
    "- Fine-tune `bert-base-uncased` on the SWAG training set\n",
    "- Try larger models: `roberta-large`, `deberta-v3-large`\n",
    "- Evaluate on out-of-domain commonsense benchmarks (HellaSwag, WinoGrande)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
