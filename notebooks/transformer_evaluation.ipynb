{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Transformer Evaluation \u2014 Commonsense Reasoning on SWAG\n",
    "\n",
    "This notebook evaluates a **pre-trained transformer** (BERT) on the [SWAG dataset](https://rowanzellers.com/swag/),\n",
    "a commonsense NLI benchmark where a model must choose the most plausible sentence continuation\n",
    "from four candidates given a premise.\n",
    "\n",
    "## Task format\n",
    "\n",
    "\n",
    "\n",
    "## Evaluation strategy\n",
    "\n",
    "We use **zero-shot scoring**: for each candidate continuation, we concatenate the premise and\n",
    "the candidate, compute the model's confidence score using the  token logit,\n",
    "and pick the highest-scoring candidate as the prediction.\n",
    "\n",
    "> **Dataset**:  \u2014 17,992 validation examples  \n",
    "> **Model**:  (zero-shot, no fine-tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# Standard library\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Hugging Face \u2014 tokenizer and model\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Central place for all hyperparameters and paths \u2014 change  to run on a\n",
    "larger subset or set it to  to evaluate the full validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "# \u2500\u2500 Paths \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "DATA_PATH = Path(\"swag/val.csv\")   # SWAG validation split\n",
    "\n",
    "# \u2500\u2500 Model \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# bert-base-uncased: 12 layers, 110M parameters \u2014 good balance of speed and accuracy\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "# \u2500\u2500 Evaluation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Limit to a subset for faster iteration; set to None to evaluate all 17 992 rows\n",
    "N_SAMPLES = 500\n",
    "MAX_SEQ_LEN = 128   # maximum token length per (premise + continuation) pair\n",
    "BATCH_SIZE  = 32    # number of pairs processed in a single forward pass\n",
    "\n",
    "# \u2500\u2500 Reproducibility \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# \u2500\u2500 Device \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Automatically use GPU if available, otherwise fall back to CPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device : {DEVICE}\")\n",
    "print(f\"Model        : {MODEL_NAME}\")\n",
    "print(f\"Eval samples : {N_SAMPLES if N_SAMPLES else 'all'}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Load & Explore the SWAG Dataset\n",
    "\n",
    "SWAG contains grounded commonsense inference examples derived from video captions.\n",
    "Each row provides:\n",
    "-  / : premise split into two parts\n",
    "- \u2013: four candidate continuations\n",
    "- : index (0\u20133) of the correct continuation\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "# Load the validation CSV\n",
    "df = pd.read_csv(DATA_PATH, index_col=0)\n",
    "\n",
    "# Randomly sample N_SAMPLES rows for faster evaluation\n",
    "if N_SAMPLES:\n",
    "    df = df.sample(n=N_SAMPLES, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# The full premise is the concatenation of sent1 and sent2\n",
    "df[\"premise\"] = df[\"sent1\"].str.strip() + \" \" + df[\"sent2\"].str.strip()\n",
    "\n",
    "# Candidate continuations are stored as separate columns\n",
    "ENDING_COLS = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n",
    "\n",
    "print(f\"Loaded {len(df):,} examples\")\n",
    "df[[\"premise\", *ENDING_COLS, \"label\"]].head(3)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Label distribution\n",
    "\n",
    "A uniform label distribution confirms the dataset is balanced \u2014\n",
    "a random baseline would achieve ~25% accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "# Plot how often each label (correct answer position) appears\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "label_counts = df[\"label\"].value_counts().sort_index()\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values, palette=\"viridis\", ax=ax)\n",
    "ax.set_xlabel(\"Correct ending index\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"SWAG \u2014 label distribution (val subset)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Random baseline accuracy (chance level)\n",
    "n_choices = len(ENDING_COLS)\n",
    "print(f\"Random baseline accuracy : {1/n_choices:.1%}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Load the Pre-trained Model\n",
    "\n",
    "We load  with a **sequence-classification head** (2 outputs).\n",
    "In zero-shot scoring, we use the logit of the positive class (index 1) as\n",
    "a proxy for how plausible the (premise, continuation) pair is.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "# Download and cache tokenizer \u2014 converts raw text into token IDs\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Download and cache model \u2014 bert-base-uncased with a 2-class classification head\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.to(DEVICE)   # move weights to GPU if available\n",
    "model.eval()       # disable dropout for deterministic inference\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "print(f\"Model loaded \u2014 {n_params:.1f}M parameters\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Zero-Shot Scoring\n",
    "\n",
    "For each example we:\n",
    "1. Build four  pairs\n",
    "2. Tokenize and batch them\n",
    "3. Run a forward pass through BERT\n",
    "4. Use **logit[1]** (positive-class score) as the plausibility score\n",
    "5. Pick the ending with the highest score as the model prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "@torch.no_grad()   # disable gradient computation \u2014 we are only doing inference\n",
    "def score_endings(premise: str, endings: list[str]) -> list[float]:\n",
    "    \"\"\"Return a plausibility score for each (premise, ending) pair.\n",
    "\n",
    "    The score is the logit of the positive class (index 1) from BERT's\n",
    "    sequence-classification head \u2014 higher means more plausible.\n",
    "\n",
    "    Args:\n",
    "        premise: The sentence context (sent1 + sent2).\n",
    "        endings: List of four candidate continuations.\n",
    "\n",
    "    Returns:\n",
    "        List of four float scores, one per ending.\n",
    "    \"\"\"\n",
    "    # Tokenize all four (premise, ending) pairs at once\n",
    "    encoding = tokenizer(\n",
    "        [premise] * len(endings),   # repeat premise for each ending\n",
    "        endings,\n",
    "        padding=True,               # pad to the longest pair in this batch\n",
    "        truncation=True,            # truncate to MAX_SEQ_LEN if needed\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        return_tensors=\"pt\",        # return PyTorch tensors\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Forward pass \u2014 shape: (num_endings, num_labels)\n",
    "    logits = model(**encoding).logits\n",
    "\n",
    "    # Logit at index 1 = positive-class score (proxy for plausibility)\n",
    "    scores = logits[:, 1].tolist()\n",
    "    return scores\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation\n",
    "\n",
    "We iterate over every example, score the four candidates, and record\n",
    "the predicted label alongside the gold label.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "predictions = []\n",
    "all_scores  = []   # store all four scores per example for later analysis\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    premise = row[\"premise\"]\n",
    "    endings = [row[col] for col in ENDING_COLS]\n",
    "\n",
    "    # Score each of the four candidate continuations\n",
    "    scores = score_endings(premise, endings)\n",
    "\n",
    "    # The predicted label is the index of the highest-scoring ending\n",
    "    pred = int(np.argmax(scores))\n",
    "    predictions.append(pred)\n",
    "    all_scores.append(scores)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "# Attach predictions back to the dataframe\n",
    "df[\"predicted\"] = predictions\n",
    "df[\"correct\"]   = df[\"predicted\"] == df[\"label\"]\n",
    "\n",
    "print(f\"Evaluated {len(df):,} examples in {elapsed:.1f}s ({elapsed/len(df)*1000:.1f} ms/example)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "\n",
    "### Overall accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "# Overall accuracy: fraction of examples where predicted == gold label\n",
    "accuracy = df[\"correct\"].mean()\n",
    "random_baseline = 1 / len(ENDING_COLS)   # 25 % for 4-choice task\n",
    "\n",
    "print(f\"Accuracy         : {accuracy:.2%}\")\n",
    "print(f\"Random baseline  : {random_baseline:.2%}\")\n",
    "print(f\"Gain over random : +{(accuracy - random_baseline):.2%}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### Per-label accuracy\n",
    "\n",
    "Checking whether accuracy varies depending on which position holds the correct answer\n",
    "reveals positional biases in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "# Compute accuracy broken down by gold-label position (0, 1, 2, 3)\n",
    "per_label = df.groupby(\"label\")[\"correct\"].mean().rename(\"accuracy\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "sns.barplot(x=per_label.index, y=per_label.values, palette=\"viridis\", ax=ax)\n",
    "ax.axhline(random_baseline, color=\"red\", linestyle=\"--\", label=f\"Random ({random_baseline:.0%})\")\n",
    "ax.axhline(accuracy, color=\"blue\", linestyle=\"--\", label=f\"Overall ({accuracy:.0%})\")\n",
    "ax.set_xlabel(\"Correct ending index\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Per-label accuracy \u2014 BERT zero-shot on SWAG\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Score distributions: correct vs. incorrect endings\n",
    "\n",
    "We expect the model to assign higher scores to correct endings.\n",
    "Overlapping distributions indicate where the model struggles.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "# Flatten all scores and tag each one as \"correct\" or \"incorrect\" ending\n",
    "score_records = []\n",
    "for i, row in df.iterrows():\n",
    "    gold = int(row[\"label\"])\n",
    "    for j, score in enumerate(all_scores[i]):\n",
    "        score_records.append({\n",
    "            \"score\": score,\n",
    "            \"ending_type\": \"correct\" if j == gold else \"incorrect\",\n",
    "        })\n",
    "score_df = pd.DataFrame(score_records)\n",
    "\n",
    "# KDE plot: correct endings should skew higher than incorrect ones\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "sns.kdeplot(\n",
    "    data=score_df, x=\"score\", hue=\"ending_type\",\n",
    "    fill=True, alpha=0.4, palette={\"correct\": \"green\", \"incorrect\": \"salmon\"},\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_xlabel(\"Plausibility score (logit)\")\n",
    "ax.set_title(\"Score distribution: correct vs. incorrect endings\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "The confusion matrix shows which gold labels the model tends to confuse with each other.\n",
    "A strong diagonal indicates consistent predictions regardless of answer position.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Build the 4x4 confusion matrix (gold label \u00d7 predicted label)\n",
    "cm = confusion_matrix(df[\"label\"], df[\"predicted\"], labels=[0, 1, 2, 3])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "    xticklabels=[\"end0\", \"end1\", \"end2\", \"end3\"],\n",
    "    yticklabels=[\"end0\", \"end1\", \"end2\", \"end3\"],\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_xlabel(\"Predicted label\")\n",
    "ax.set_ylabel(\"Gold label\")\n",
    "ax.set_title(\"Confusion matrix \u2014 BERT zero-shot on SWAG\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 7. Error Analysis\n",
    "\n",
    "Inspecting model errors reveals systematic failure patterns and\n",
    "guides further improvement (e.g., fine-tuning, prompt engineering).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "# Show the 5 errors where the model was most confidently wrong\n",
    "# Confidence = score of the (wrong) predicted ending\n",
    "errors = df[~df[\"correct\"]].copy()\n",
    "\n",
    "# Retrieve predicted score for each error (the score of the predicted ending)\n",
    "errors[\"pred_score\"] = [\n",
    "    all_scores[i][predictions[i]] for i in errors.index\n",
    "]\n",
    "\n",
    "# Sort by highest predicted score (most confident wrong predictions)\n",
    "worst = errors.nlargest(5, \"pred_score\")[[\"premise\", \"predicted\", \"label\", \"pred_score\", *ENDING_COLS]]\n",
    "\n",
    "print(\"=== Top-5 most confident errors ===\")\n",
    "for _, row in worst.iterrows():\n",
    "    gold_idx = int(row[\"label\"])\n",
    "    pred_idx = int(row[\"predicted\"])\n",
    "    print(f\"\nPremise   : {row['premise']}\")\n",
    "    print(f\"Gold      : [{gold_idx}] {row[ENDING_COLS[gold_idx]]}\")\n",
    "    print(f\"Predicted : [{pred_idx}] {row[ENDING_COLS[pred_idx]]} (score={row['pred_score']:.2f})\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Model |  (zero-shot) |\n",
    "| Dataset | SWAG validation set |\n",
    "| Eval samples |  |\n",
    "| **Accuracy** | _run cells above_ |\n",
    "| Random baseline | 25.0% |\n",
    "\n",
    "### Key takeaways\n",
    "\n",
    "- **Zero-shot BERT** already outperforms the random baseline on commonsense reasoning,\n",
    "  demonstrating that pre-training on large corpora captures implicit world knowledge.\n",
    "- Score distributions show a meaningful separation between correct and incorrect endings,\n",
    "  but significant overlap remains \u2014 this is expected for a zero-shot approach.\n",
    "- **Fine-tuning BERT on SWAG** (as in the original paper) reaches ~86% accuracy,\n",
    "  compared to human performance of ~88%.\n",
    "\n",
    "### Next steps\n",
    "\n",
    "- Fine-tune  on the SWAG training set\n",
    "- Try larger models: , \n",
    "- Evaluate on out-of-domain commonsense benchmarks (HellaSwag, WinoGrande)\n"
   ]
  }
 ]
}